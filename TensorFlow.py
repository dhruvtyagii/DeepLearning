# -*- coding: utf-8 -*-
"""RedCarpetUpDhruvTyagi.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-IWe9uz_KyX6ldqhNL-IWMdWahH43ZyX
"""

import pandas as pd
import tensorflow as tf
import numpy as np


cars_df = pd.read_csv('cars.csv')


cars_df['all_utilities'] = cars_df['Sound'] + cars_df['Cruise'] + cars_df['Leather']

cars_df['all_utilities'] = [ 1 if x == 3 else 0 for x in cars_df['all_utilities']]

cars_df.drop(['Cruise' , 'Sound' , 'Leather'] , axis = 1 , inplace = True)

X = cars_df.drop(['all_utilities'] , axis = 1)
y = cars_df['all_utilities']
print(y.head())
cars_df['Price'] = cars_df['Price'].astype(float)
cars_df['Mileage'] = cars_df['Mileage'].astype(float)
cars_df['Cylinder'] = cars_df['Cylinder'].astype(float)
cars_df['Liter'] = cars_df['Liter'].astype(float)
cars_df['Doors'] = cars_df['Doors'].astype(float)
from sklearn.model_selection import train_test_split
from sklearn.base import TransformerMixin, BaseEstimator
from sklearn.preprocessing import StandardScaler , OneHotEncoder
from sklearn.pipeline import Pipeline , FeatureUnion
class DataFrameSelector(BaseEstimator,TransformerMixin):
    def __init__(self,attribute):
        self.attributes = attribute
    def fit(self, X , y = None):
        return self
    def transform(self,X):
        return X[self.attributes].values
num_attribs = ['Price' , 'Mileage' , 'Cylinder' , 'Liter' , 'Doors']
cat_attribs = ['Make' , 'Model' , 'Trim' , 'Type']

num_pipeline = Pipeline([('dfs',DataFrameSelector(num_attribs)),('std',StandardScaler())])
cat_pipeline = Pipeline([('dfs',DataFrameSelector(cat_attribs)),('encode',OneHotEncoder(sparse = False))] )
full_pipeline = FeatureUnion(transformer_list = [('num',num_pipeline),('cat',cat_pipeline)])
X_train,X_test,y_train,y_test = train_test_split( X , y , test_size=0.33 , random_state=42 )
X_train_Boosted , X_test_Boosted , y_train_Boosted , y_test_Boosted  = train_test_split( X , y , test_size=0.33,
                                                                                           random_state=42)
X_train = full_pipeline.fit_transform(X_train)
X_test = full_pipeline.fit_transform(X_test)
y_train = y_train.as_matrix()

y_train = y_train.tolist()


y_train1 = [[1.0,0.0] if x == 1 else [0.0,1.0] for x in y_train]
y_train =  np.asarray(y_train1)
y_test1 = [[1.0,0.0] if x == 1 else [0.0,1.0] for x in y_test]
y_test = np.asarray(y_test1)
print(y_train.shape , X_train.shape)
print(y_train)

# LOGISTIC REGRESSION USING SIGMOID CROSS ENTROPY


# Training and evaluation input functions.

X = tf.placeholder(tf.float32, [None , 95])

# Since this is a binary classification problem,
# Y can take only 2 values.
Y = tf.placeholder(tf.float32, [None,2])

# Trainable Variable Weights
W = tf.Variable(tf.zeros([95 , 2]))

# Trainable Variable Bias
b = tf.Variable(tf.zeros([2]))

Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(X, W), b))


cost = tf.nn.sigmoid_cross_entropy_with_logits(
    logits=Y_hat, labels=Y)

epochs = 500
optimizer = tf.train.GradientDescentOptimizer(
    learning_rate=0.035).minimize(cost)


init = tf.global_variables_initializer()

with tf.Session() as sess:

    sess.run(init)


    cost_history, accuracy_history = [], []


    for epoch in range(epochs):
        cost_per_epoch = 0


        sess.run(optimizer, feed_dict={X: X_train, Y: y_train})


        c = sess.run(cost, feed_dict={X: X_train, Y: y_train})


        correct_prediction = tf.equal(tf.argmax(Y_hat, 1),
                                      tf.argmax(Y, 1))
        accuracy = tf.reduce_mean(tf.cast(correct_prediction,
                                          tf.float32))


        cost_history.append(sum(sum(c)))
        accuracy_history.append(accuracy.eval({X: X_train, Y: y_train}) * 100)


        if epoch % 100 == 0 and epoch != 0:
            print("Epoch " + str(epoch) + " Cost: "
                  + str(cost_history[-1]))

    Weight = sess.run(W)
    Bias = sess.run(b)


    correct_prediction = tf.equal(tf.argmax(Y_hat, 1),
                                  tf.argmax(Y, 1))
    accuracy = tf.reduce_mean(tf.cast(correct_prediction,
                                      tf.float32))
    print("\nAccuracy:", accuracy_history[-1], "%")


# Random Forest
from tensorflow.contrib.tensor_forest.python import tensor_forest
from tensorflow.python.ops import resources
num_steps = 500
batch_size = 538
num_classes = 2
num_features = 95
num_trees = 10
max_nodes = 1000

X = tf.placeholder(tf.float32,[None, 95])

Y = tf.placeholder(tf.float32,[None,2])

hparams = tensor_forest.ForestHParams(num_classes=num_classes,
                                      num_features=num_features,
                                      num_trees=num_trees,
                                      max_nodes=max_nodes).fill()

forest_graph = tensor_forest.RandomForestGraphs(hparams)

train_op = forest_graph.training_graph(X, Y)
loss_op = forest_graph.training_loss(X, Y)


infer_op, _, _ = forest_graph.inference_graph(X)
correct_prediction = tf.equal(tf.argmax(infer_op, 0), tf.cast(Y, tf.int64))
accuracy_op = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

init_vars = tf.group(tf.global_variables_initializer(),
    resources.initialize_resources(resources.shared_resources()))

sess = tf.Session()

sess.run(init_vars)

for i in range(1, num_steps + 1):

    l = sess.run([train_op, loss_op], feed_dict={X: X_train, Y: y_train})




# Gradient Boosting

fc = tf.feature_column


def one_hot_cat_column(feature_name, vocab):
    return fc.indicator_column(
        fc.categorical_column_with_vocabulary_list(feature_name,
                                                   vocab))


feature_columns = []
for feature_name in cat_attribs:
    # Need to one-hot encode categorical features.
    vocabulary = cars_df[feature_name].unique()

    feature_columns.append(one_hot_cat_column(feature_name, vocabulary))

for feature_name in num_attribs:
    feature_columns.append(fc.numeric_column(feature_name,
                                             dtype=tf.float32))

NUM_EXAMPLES = len(y_train)

def make_input_fn(X, y, n_epochs=None, shuffle=True):
  def input_fn():
    dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))
    if shuffle:
      dataset = dataset.shuffle(NUM_EXAMPLES)
    # For training, cycle thru dataset as many times as need (n_epochs=None).
    dataset = dataset.repeat(n_epochs)
    # In memory training doesn't use batching.
    dataset = dataset.batch(NUM_EXAMPLES)
    return dataset
  return input_fn

# Training and evaluation input functions.
train_input_fn = make_input_fn(X_train_Boosted, y_train_Boosted)
eval_input_fn = make_input_fn(X_test_Boosted, y_test_Boosted, shuffle=False, n_epochs=1)
n_batches = 1
est = tf.estimator.BoostedTreesClassifier(feature_columns,
                                          n_batches_per_layer=n_batches)
est.train(train_input_fn, max_steps=100)


results = est.evaluate(eval_input_fn)
print('Accuracy : ', results['accuracy'])
print('Dummy model: ', results['accuracy_baseline'])